{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fb3c30",
   "metadata": {},
   "source": [
    "## Project Group 1; Naeim's crude code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48834a48",
   "metadata": {},
   "source": [
    "### Setting up a Spark Session to work with structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f36790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1969b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the code on local machine or on server\n",
    "server_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bce2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:54:29 WARN Utils: Your hostname, nanook resolves to a loopback address: 127.0.1.1; using 192.168.1.69 instead (on interface enp5s0)\n",
      "22/03/15 17:54:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/naeim/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/15 17:54:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "if server_mode:\n",
    "    #New API\n",
    "    spark_session = SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"spark://192.168.2.74:7077\") \\\n",
    "            .appName(\"Project_G1_Naeim\")\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "            .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "            .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "            .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "            .config(\"spark.executor.cores\",1)\\\n",
    "            .config(\"spark.driver.port\",9998)\\\n",
    "            .config(\"spark.blockManager.port\",10005)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # Old API (RDD)\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "    # spark_context.setLogLevel(\"INFO\")\n",
    "    \n",
    "else:\n",
    "    # local version, deactivate later!\n",
    "    spark_session = SparkSession.builder.appName('Project_G1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07711204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_G1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4354878df0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session # to get some info about the Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145c744",
   "metadata": {},
   "source": [
    "### Check the existing source files and their names in the remote directories before downloading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563f4dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files with csv extension: 19\n",
      "['taxi-zone-lookup.csv', 'uber-raw-data-apr14.csv', 'uber-raw-data-aug14.csv', 'uber-raw-data-jul14.csv', 'uber-raw-data-jun14.csv']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "\n",
    "urls = ['https://github.com/fivethirtyeight/uber-tlc-foil-response/blob/master/uber-trip-data/',\n",
    "        'https://github.com/fivethirtyeight/uber-tlc-foil-response/blob/master/other-FHV-data/',\n",
    "        'https://github.com/fivethirtyeight/uber-tlc-foil-response/']\n",
    "\n",
    "exts = ['csv'] #, 'xlsx'\n",
    "\n",
    "def get_content(url, ext=''):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return [url + '/' + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
    "file_names = []\n",
    "\n",
    "for url in urls:\n",
    "    for ext in exts:\n",
    "        for file in get_content(url, ext):\n",
    "            file_names.append(os.path.basename(file))\n",
    "\n",
    "print(f'Number of files with csv extension: {len(file_names)}')\n",
    "print(file_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07153807",
   "metadata": {},
   "source": [
    "### Download source files in local file system before working with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3449c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository is ALREADY downloaded!\n"
     ]
    }
   ],
   "source": [
    "# !pip install gitpython\n",
    "from git import Repo\n",
    "if not os.path.isdir('DATA') or len(os.listdir('DATA')) == 0:\n",
    "    git_url = 'https://github.com/fivethirtyeight/uber-tlc-foil-response.git'\n",
    "    Repo.clone_from(git_url, 'Git_repo');\n",
    "    print('The repository is downloaded!')\n",
    "else:\n",
    "    print('The repository is ALREADY downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf18dc",
   "metadata": {},
   "source": [
    "### Keeping useful files from git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d38724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['csv'] files already extracted!\n",
      "Number of new ['csv'] extracted files: 19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src = r'Git_repo'\n",
    "if os.path.isdir('Git_repo'):\n",
    "    os.system(\"rm -rf DATA\")\n",
    "    os.makedirs('DATA')\n",
    "    print(\"Old DATA directory deleted!\")\n",
    "    dest = r'DATA'\n",
    "\n",
    "    for path, subdirs, files in os.walk(src):\n",
    "        if '.git' in subdirs:\n",
    "            subdirs.remove('.git')\n",
    "        for name in files:\n",
    "            if name.endswith('.csv'): # or name.endswith('.xlsx'):\n",
    "                filename = os.path.join(path, name)\n",
    "                shutil.copy2(filename, dest)\n",
    "else:\n",
    "    print(f\"{exts} files already extracted!\")\n",
    "\n",
    "if os.path.isdir('DATA'):\n",
    "    path = os.getcwd() + '/DATA'\n",
    "    list_dir = os.listdir(path)\n",
    "    count = 0\n",
    "    for file in list_dir:\n",
    "        if file.endswith(exts[0]):\n",
    "            count += 1\n",
    "    print(f\"Number of new {exts} extracted files: {count}\")\n",
    "os.system(\"rm -rf Git_repo\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380e56f",
   "metadata": {},
   "source": [
    "### Load the CSV files from source folder, and call show() to verify the data is loaded to RAM correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1b3c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Laodig data in Spark & writing in HDFS\n",
    "df_list = {}\n",
    "for file in file_names:\n",
    "    # spark_file = 'df_'+ (os.path.splitext(file)[0]).split('-')[-1]\n",
    "    # df_list[file] = spark_session.read.csv('DATA/'+file, header=True, inferSchema=True)\n",
    "    df_list[file] = spark_session.read.option(\"header\",\"true\").csv('DATA/'+file)\n",
    "\n",
    "# df_list.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c8b251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark dataframe exists\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "if df_list[file_names[0]] is not None and \\\n",
    "                isinstance(df_list[file_names[0]], \\\n",
    "                    pyspark.sql.dataframe.DataFrame):\n",
    "    print(\"Spark dataframe exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e03dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('taxi-zone-lookup.csv', pyspark.sql.dataframe.DataFrame)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names[0], type(df_list[file_names[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ede443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|LocationID|Borough|          Zone|\n",
      "+----------+-------+--------------+\n",
      "|         1|    EWR|Newark Airport|\n",
      "|         2| Queens|   Jamaica Bay|\n",
      "+----------+-------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list[file_names[0]].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff380a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list[file_names[0]].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8321d34",
   "metadata": {},
   "source": [
    "### Pre-processing (Null values, Seperating date and time, Unified dataframe for related data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a694e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is inspired by 'pault' @ stackoverflow\n",
    "# Pyspark - Calculate number of null values in each dataframe column\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "from functools import reduce\n",
    "\n",
    "def count_null_values_in_df(any_df):\n",
    "    \n",
    "    df_agg = any_df.agg(*[func.count(func.when(func.isnull(c), c)).alias(c) for c in any_df.columns])\n",
    "    null_table = reduce(lambda a, b: a.union(b),(\n",
    "        df_agg.select(func.lit(c).alias(\"Column\"), func.col(c).alias(\"Nbr of Null\")) \n",
    "        for c in df_agg.columns))\n",
    "    \n",
    "    null_table.show()\n",
    "    print(f'Initial number of rows: {any_df.count()}')\n",
    "    print(\"************\\n\")\n",
    "    \n",
    "    any_df = any_df.na.drop(how='all') # to drop any row with all values as Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7ffaa8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: uber-raw-data-apr14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|Date/Time|          0|\n",
      "|      Lat|          0|\n",
      "|      Lon|          0|\n",
      "|     Base|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 564516\n",
      "************\n",
      "\n",
      "2: uber-raw-data-aug14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|Date/Time|          0|\n",
      "|      Lat|          0|\n",
      "|      Lon|          0|\n",
      "|     Base|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 829275\n",
      "************\n",
      "\n",
      "3: uber-raw-data-jul14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|Date/Time|          0|\n",
      "|      Lat|          0|\n",
      "|      Lon|          0|\n",
      "|     Base|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 796121\n",
      "************\n",
      "\n",
      "4: uber-raw-data-jun14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|Date/Time|          0|\n",
      "|      Lat|          0|\n",
      "|      Lon|          0|\n",
      "|     Base|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 663844\n",
      "************\n",
      "\n",
      "5: uber-raw-data-may14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|Date/Time|          0|\n",
      "|      Lat|          0|\n",
      "|      Lon|          0|\n",
      "|     Base|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 652435\n",
      "************\n",
      "\n",
      "6: uber-raw-data-sep14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|Date/Time|          0|\n",
      "|      Lat|          0|\n",
      "|      Lon|          0|\n",
      "|     Base|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 1028136\n",
      "************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' in key:\n",
    "        print(f'{x}: {key}')\n",
    "        count_null_values_in_df(df_list[key])   \n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeab903",
   "metadata": {},
   "source": [
    "### Preparing Uber raw data\n",
    "Uber data (4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb5239a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date/Time', 'Lat', 'Lon', 'Base']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = df_list['uber-raw-data-apr14.csv'].schema.names\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402553b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 92:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+----+\n",
      "|Date/Time|Lat|Lon|Base|\n",
      "+---------+---+---+----+\n",
      "+---------+---+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "sc = spark_session.sparkContext\n",
    "\n",
    "col_names = df_list['uber-raw-data-apr14.csv'].schema.names\n",
    "mySchema = StructType([StructField(c, StringType()) for c in col_names])\n",
    "uber_raw_data = SparkSession(sc).createDataFrame(data=[], schema=mySchema)\n",
    "uber_raw_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31056fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files combined: 6\n",
      "Total number of rows in unified dataframe: 4534327\n"
     ]
    }
   ],
   "source": [
    "manual_rows = 0\n",
    "number_of_files = 0\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' in key:\n",
    "        uber_raw_data = uber_raw_data.union(df_list[key])\n",
    "        manual_rows += df_list[key].count()\n",
    "        number_of_files += 1\n",
    "\n",
    "print(f\"Total number of files combined: {number_of_files}\")\n",
    "print(f'Total number of rows in unified dataframe: {manual_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f933cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:===============================================>       (24 + 4) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows match in the unified file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if (uber_raw_data.count() == manual_rows):\n",
    "    print(\"The number of rows match in the unified file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb389091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/uber-trip-data'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "uber_unified_file = path+\"uber_raw_data.parquet\"\n",
    "uber_raw_data.write.mode(\"overwrite\").parquet(uber_unified_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6306d5",
   "metadata": {},
   "source": [
    "### To check if downloaded files exists in HDFS system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942573a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/uber-trip-data/'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "path = path + 'uber_raw_data.parquet'\n",
    "print(fs.exists(sc._jvm.org.apache.hadoop.fs.Path(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798757e9",
   "metadata": {},
   "source": [
    "### To read parquet file use the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56d53678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date/Time: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Lon: string (nullable = true)\n",
      " |-- Base: string (nullable = true)\n",
      "\n",
      "+----------------+-------+--------+------+\n",
      "|       Date/Time|    Lat|     Lon|  Base|\n",
      "+----------------+-------+--------+------+\n",
      "|9/1/2014 0:01:00|40.2201|-74.0021|B02512|\n",
      "|9/1/2014 0:01:00|  40.75|-74.0027|B02512|\n",
      "|9/1/2014 0:03:00|40.7559|-73.9864|B02512|\n",
      "+----------------+-------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4534327"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_raw_data_read_test = spark_session.read.parquet(uber_unified_file)\n",
    "uber_raw_data_read_test.printSchema()\n",
    "uber_raw_data_read_test.show(3)\n",
    "uber_raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae0a9e",
   "metadata": {},
   "source": [
    "### Preparing For-Hire Vehicle (FHV) data\n",
    "FHV companies (10 files of raw data on pickups from 10 FHV companies. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e080354",
   "metadata": {},
   "source": [
    "#### Drop Null columns manually for each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c02e73f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: taxi-zone-lookup.csv\n",
      "[('LocationID', 'string'), ('Borough', 'string'), ('Zone', 'string')]\n",
      "+----------+-----------+\n",
      "|    Column|Nbr of Null|\n",
      "+----------+-----------+\n",
      "|LocationID|          0|\n",
      "|   Borough|          0|\n",
      "|      Zone|          0|\n",
      "+----------+-----------+\n",
      "\n",
      "Initial number of rows: 265\n",
      "************\n",
      "\n",
      "2: American_B01362.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PICK UP ADDRESS', 'string'), ('_c3', 'string'), ('_c4', 'string'), ('_c5', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:55:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c3\n",
      "Expected: _c3 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/American_B01362.csv\n",
      "22/03/15 17:55:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c4\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/American_B01362.csv\n",
      "22/03/15 17:55:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c5\n",
      "Expected: _c5 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/American_B01362.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           DATE|          0|\n",
      "|           TIME|          0|\n",
      "|PICK UP ADDRESS|          0|\n",
      "|            _c3|      91712|\n",
      "|            _c4|      91712|\n",
      "|            _c5|      91712|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 91712\n",
      "************\n",
      "\n",
      "3: Carmel_B00256.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('PU_Adress', 'string'), ('Base_No', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|     Date|          0|\n",
      "|     Time|          0|\n",
      "|PU_Adress|          0|\n",
      "|  Base_No|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 256519\n",
      "************\n",
      "\n",
      "4: Dial7_B00887.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('State', 'string'), ('PuFrom', 'string'), ('Address', 'string'), ('Street', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "| Column|Nbr of Null|\n",
      "+-------+-----------+\n",
      "|   Date|          0|\n",
      "|   Time|          0|\n",
      "|  State|          0|\n",
      "| PuFrom|      37030|\n",
      "|Address|          0|\n",
      "| Street|          0|\n",
      "+-------+-----------+\n",
      "\n",
      "Initial number of rows: 194992\n",
      "************\n",
      "\n",
      "5: Diplo_B01196.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('PU_Address', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|    Column|Nbr of Null|\n",
      "+----------+-----------+\n",
      "|      Date|          0|\n",
      "|      Time|          0|\n",
      "|PU_Address|          0|\n",
      "+----------+-----------+\n",
      "\n",
      "Initial number of rows: 98550\n",
      "************\n",
      "\n",
      "6: Federal_02216.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('PU_Address2', 'string'), ('DO_Address', 'string'), ('Routing Details', 'string'), ('PU_Address5', 'string'), ('Status', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:55:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: PU_Address\n",
      " Schema: PU_Address2\n",
      "Expected: PU_Address2 but found: PU_Address\n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Federal_02216.csv\n",
      "22/03/15 17:55:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: PU_Address\n",
      " Schema: PU_Address5\n",
      "Expected: PU_Address5 but found: PU_Address\n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Federal_02216.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           Date|          0|\n",
      "|           Time|          0|\n",
      "|    PU_Address2|          7|\n",
      "|     DO_Address|          9|\n",
      "|Routing Details|          0|\n",
      "|    PU_Address5|          0|\n",
      "|         Status|          0|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 276\n",
      "************\n",
      "\n",
      "7: Firstclass_B01536.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PICK UP ADDRESS', 'string')]\n",
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           DATE|          0|\n",
      "|           TIME|          0|\n",
      "|PICK UP ADDRESS|          0|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 166769\n",
      "************\n",
      "\n",
      "8: Highclass_B01717.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PU_Address', 'string')]\n",
      "+----------+-----------+\n",
      "|    Column|Nbr of Null|\n",
      "+----------+-----------+\n",
      "|      DATE|          0|\n",
      "|      TIME|          0|\n",
      "|PU_Address|          0|\n",
      "+----------+-----------+\n",
      "\n",
      "Initial number of rows: 151925\n",
      "************\n",
      "\n",
      "9: Lyft_B02510.csv\n",
      "[('time_of_trip', 'string'), ('start_lat', 'string'), ('start_lng', 'string'), ('_c3', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:55:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c3\n",
      "Expected: _c3 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Lyft_B02510.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|      Column|Nbr of Null|\n",
      "+------------+-----------+\n",
      "|time_of_trip|          0|\n",
      "|   start_lat|          1|\n",
      "|   start_lng|          1|\n",
      "|         _c3|     266503|\n",
      "+------------+-----------+\n",
      "\n",
      "Initial number of rows: 267701\n",
      "************\n",
      "\n",
      "10: Prestige_B01338.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PICK UP ADDRESS', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           DATE|          0|\n",
      "|           TIME|          0|\n",
      "|PICK UP ADDRESS|          0|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 320641\n",
      "************\n",
      "\n",
      "11: Skyline_B00111.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('    Street_Address ', 'string'), ('    City_State ', 'string'), ('_c4', 'string'), ('_c5', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c4\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Skyline_B00111.csv\n",
      "22/03/15 17:56:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c5\n",
      "Expected: _c5 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Skyline_B00111.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|             Column|Nbr of Null|\n",
      "+-------------------+-----------+\n",
      "|               Date|          0|\n",
      "|               Time|          0|\n",
      "|    Street_Address |          0|\n",
      "|        City_State |          0|\n",
      "|                _c4|     127696|\n",
      "|                _c5|     127696|\n",
      "+-------------------+-----------+\n",
      "\n",
      "Initial number of rows: 127696\n",
      "************\n",
      "\n",
      "12: other-FHV-data-jan-aug-2015.csv\n",
      "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string'), ('_c4', 'string'), ('_c5', 'string'), ('_c6', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c0\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/15 17:56:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c1\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/15 17:56:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c2\n",
      "Expected: _c2 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/15 17:56:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c3\n",
      "Expected: _c3 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/15 17:56:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c4\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/15 17:56:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c5\n",
      "Expected: _c5 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/15 17:56:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: \n",
      " Schema: _c6\n",
      "Expected: _c6 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|Column|Nbr of Null|\n",
      "+------+-----------+\n",
      "|   _c0|      26186|\n",
      "|   _c1|          5|\n",
      "|   _c2|          5|\n",
      "|   _c3|      26187|\n",
      "|   _c4|          5|\n",
      "|   _c5|          5|\n",
      "|   _c6|          5|\n",
      "+------+-----------+\n",
      "\n",
      "Initial number of rows: 26187\n",
      "************\n",
      "\n",
      "13: Uber-Jan-Feb-FOIL.csv\n",
      "[('dispatching_base_number', 'string'), ('date', 'string'), ('active_vehicles', 'string'), ('trips', 'string')]\n",
      "+--------------------+-----------+\n",
      "|              Column|Nbr of Null|\n",
      "+--------------------+-----------+\n",
      "|dispatching_base_...|          0|\n",
      "|                date|          0|\n",
      "|     active_vehicles|          0|\n",
      "|               trips|          0|\n",
      "+--------------------+-----------+\n",
      "\n",
      "Initial number of rows: 354\n",
      "************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' not in key:\n",
    "        print(f'{x}: {key}')\n",
    "        print(df_list[key].dtypes)\n",
    "        count_null_values_in_df(df_list[key]) \n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c60971a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   PuFrom|\n",
      "+---------+\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|   QUEENS|\n",
      "|MANHATTAN|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list['Dial7_B00887.csv'].select('PuFrom').show(10)\n",
    "df_list['Dial7_B00887.csv'] = df_list['Dial7_B00887.csv']\\\n",
    "                        .na.fill('UNKOWN')\n",
    "df_list['Dial7_B00887.csv'].filter(df_list['Dial7_B00887.csv']\\\n",
    "                        .PuFrom.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "912a2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+----+----+----+\n",
      "|    DATE|       TIME|     PICK UP ADDRESS| _c3| _c4| _c5|\n",
      "+--------+-----------+--------------------+----+----+----+\n",
      "|7/1/2014|12:00:00 AM| 874 E 139th St M...|null|null|null|\n",
      "|7/1/2014|12:01:00 AM| 628 E 141st St M...|null|null|null|\n",
      "|7/1/2014|12:01:00 AM| 601 E 156th St S...|null|null|null|\n",
      "|7/1/2014|12:01:00 AM| 708 E 138th St M...|null|null|null|\n",
      "|7/1/2014|12:02:00 AM| 700 E 140th St M...|null|null|null|\n",
      "+--------+-----------+--------------------+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----------+--------------------+\n",
      "|    DATE|       TIME|     PICK UP ADDRESS|\n",
      "+--------+-----------+--------------------+\n",
      "|7/1/2014|12:00:00 AM| 874 E 139th St M...|\n",
      "|7/1/2014|12:01:00 AM| 628 E 141st St M...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DATE, TIME, PICK UP ADDRESS, , , \n",
      " Schema: DATE, TIME, PICK UP ADDRESS, _c3, _c4, _c5\n",
      "Expected: _c3 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/American_B01362.csv\n"
     ]
    }
   ],
   "source": [
    "df_list['American_B01362.csv'].show(5)\n",
    "columns_to_drop = ['_c3','_c4','_c5']\n",
    "df_list['American_B01362.csv'] = \\\n",
    "    df_list['American_B01362.csv'].drop(*columns_to_drop)\n",
    "df_list['American_B01362.csv'].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "672d18d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+----+\n",
      "|   time_of_trip|start_lat|start_lng| _c3|\n",
      "+---------------+---------+---------+----+\n",
      "|  9/4/2014 9:51| 40.64705|-73.77988|null|\n",
      "|8/27/2014 21:13| 40.74916|-73.98373|null|\n",
      "+---------------+---------+---------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------+---------+---------+\n",
      "|   time_of_trip|start_lat|start_lng|\n",
      "+---------------+---------+---------+\n",
      "|  9/4/2014 9:51| 40.64705|-73.77988|\n",
      "|8/27/2014 21:13| 40.74916|-73.98373|\n",
      "+---------------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: time_of_trip, start_lat, start_lng, \n",
      " Schema: time_of_trip, start_lat, start_lng, _c3\n",
      "Expected: _c3 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Lyft_B02510.csv\n"
     ]
    }
   ],
   "source": [
    "df_list['Lyft_B02510.csv'].show(2)\n",
    "df_list['Lyft_B02510.csv'] = \\\n",
    "    df_list['Lyft_B02510.csv'].drop('_c3')\n",
    "df_list['Lyft_B02510.csv'].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e799102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Time,     Street_Address ,     City_State , , \n",
      " Schema: Date, Time,     Street_Address ,     City_State , _c4, _c5\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Skyline_B00111.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+--------------------+----+----+\n",
      "|    Date|          Time|     Street_Address |         City_State | _c4| _c5|\n",
      "+--------+--------------+--------------------+--------------------+----+----+\n",
      "|7/1/2014|    20:27     |    622 THIRD AV ...|     M           ...|null|null|\n",
      "|7/1/2014|    21:04     |     E 77TH ST   ...|     M           ...|null|null|\n",
      "+--------+--------------+--------------------+--------------------+----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "|    Date|          Time|     Street_Address |         City_State |\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "|7/1/2014|    20:27     |    622 THIRD AV ...|     M           ...|\n",
      "|7/1/2014|    21:04     |     E 77TH ST   ...|     M           ...|\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list['Skyline_B00111.csv'].show(2)\n",
    "df_list['Skyline_B00111.csv'] = \\\n",
    "    df_list['Skyline_B00111.csv'].drop(*columns_to_drop)\n",
    "df_list['Skyline_B00111.csv'].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c54e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['_c0','_c3']\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "    df_list['other-FHV-data-jan-aug-2015.csv'].drop(*columns_to_drop)\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "    df_list['other-FHV-data-jan-aug-2015.csv'].na.drop(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40195b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------+---------------+------------------+\n",
      "|        _c1|      _c2|         _c4|            _c5|               _c6|\n",
      "+-----------+---------+------------+---------------+------------------+\n",
      "|Base Number|Base Name|Pick Up Date|Number of Trips|Number of Vehicles|\n",
      "+-----------+---------+------------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , \n",
      " Schema: _c1, _c2, _c4, _c5, _c6\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n"
     ]
    }
   ],
   "source": [
    "df_list['other-FHV-data-jan-aug-2015.csv'].show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c254ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , \n",
      " Schema: _c1, _c2, _c4, _c5, _c6\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/15 17:56:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , \n",
      " Schema: _c1, _c2, _c4, _c5, _c6\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "|Base_Number|           Base_Name|Pick_Up_Date|Number_of_Trips|Number_of_Vehicles|\n",
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "|     B00013|   LOVE CORP CAR INC|  01/01/2015|            26 |               17 |\n",
      "|     B00014| NY ONE CORP CAR INC|  01/01/2015|            45 |               24 |\n",
      "|     B00029|COMMUNITY CAR SVC...|  01/01/2015|           731 |               36 |\n",
      "|     B00053| CHARGE AND RIDE INC|  01/01/2015|            10 |                9 |\n",
      "|     B00095|LIBERTY CAR SERVI...|  01/01/2015|           814 |               62 |\n",
      "|     B00221|PROFESSIONAL CAR ...|  01/01/2015|           220 |               46 |\n",
      "|     B00227|PARK WEST EXEC. S...|  01/01/2015|            36 |               28 |\n",
      "|     B00248|YELLOWSTONE TRANS...|  01/01/2015|         1,137 |              106 |\n",
      "|     B00254|   XYZ TWO WAY RADIO|  01/01/2015|           236 |              103 |\n",
      "|     B00280|FLEET RADIO DISPA...|  01/01/2015|            47 |               29 |\n",
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "        df_list['other-FHV-data-jan-aug-2015.csv'].withColumnRenamed(\"_c1\", \"Base_Number\")\\\n",
    "       .withColumnRenamed(\"_c2\", \"Base_Name\").withColumnRenamed(\"_c4\", \"Pick_Up_Date\")\\\n",
    "       .withColumnRenamed(\"_c5\", \"Number_of_Trips\").withColumnRenamed(\"_c6\", \"Number_of_Vehicles\")\n",
    "\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "        SparkSession(sc).createDataFrame(df_list['other-FHV-data-jan-aug-2015.csv']\\\n",
    "                              .tail(df_list['other-FHV-data-jan-aug-2015.csv'].count()-1)\\\n",
    "                              ,df_list['other-FHV-data-jan-aug-2015.csv'].schema)\n",
    "\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d839c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list['Lyft_B02510.csv'] = df_list['Lyft_B02510.csv'].na.drop()\n",
    "df_list['Federal_02216.csv'] = df_list['Federal_02216.csv']\\\n",
    "                        .na.fill('UNKOWN')\n",
    "\n",
    "df_list['American_B01362.csv'] = \\\n",
    "        df_list['American_B01362.csv'].withColumnRenamed(\"PICK UP ADDRESS\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Prestige_B01338.csv'] = \\\n",
    "        df_list['Prestige_B01338.csv'].withColumnRenamed(\"PICK UP ADDRESS\", \"PICK_UP_ADDRESS\")\n",
    "\n",
    "df_list['Firstclass_B01536.csv'] = \\\n",
    "        df_list['Firstclass_B01536.csv'].withColumnRenamed(\"PICK UP ADDRESS\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Federal_02216.csv'] = \\\n",
    "        df_list['Federal_02216.csv'].withColumnRenamed(\"Routing Details\", \"Routing_Details\")\n",
    "\n",
    "df_list['Skyline_B00111.csv'] = \\\n",
    "        df_list['Skyline_B00111.csv'].withColumnRenamed(\"    Street_Address \", \"Street_Address\")\n",
    "df_list['Skyline_B00111.csv'] = \\\n",
    "        df_list['Skyline_B00111.csv'].withColumnRenamed(\"    City_State \", \"City_State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89d838d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: taxi-zone-lookup.csv\n",
      "[('LocationID', 'string'), ('Borough', 'string'), ('Zone', 'string')]\n",
      "+----------+-------+--------------------+\n",
      "|LocationID|Borough|                Zone|\n",
      "+----------+-------+--------------------+\n",
      "|         1|    EWR|      Newark Airport|\n",
      "|         2| Queens|         Jamaica Bay|\n",
      "|         3|  Bronx|Allerton/Pelham G...|\n",
      "+----------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----------+\n",
      "|    Column|Nbr of Null|\n",
      "+----------+-----------+\n",
      "|LocationID|          0|\n",
      "|   Borough|          0|\n",
      "|      Zone|          0|\n",
      "+----------+-----------+\n",
      "\n",
      "Initial number of rows: 265\n",
      "************\n",
      "\n",
      "2: American_B01362.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PICK_UP_ADDRESS', 'string')]\n",
      "+--------+-----------+--------------------+\n",
      "|    DATE|       TIME|     PICK_UP_ADDRESS|\n",
      "+--------+-----------+--------------------+\n",
      "|7/1/2014|12:00:00 AM| 874 E 139th St M...|\n",
      "|7/1/2014|12:01:00 AM| 628 E 141st St M...|\n",
      "|7/1/2014|12:01:00 AM| 601 E 156th St S...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           DATE|          0|\n",
      "|           TIME|          0|\n",
      "|PICK_UP_ADDRESS|          0|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 91712\n",
      "************\n",
      "\n",
      "3: Carmel_B00256.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('PU_Adress', 'string'), ('Base_No', 'string')]\n",
      "+--------+----+---------------+-------+\n",
      "|    Date|Time|      PU_Adress|Base_No|\n",
      "+--------+----+---------------+-------+\n",
      "|7/1/2014|0:00|260 W 44 St NYC| B00256|\n",
      "|7/1/2014|0:00|125 W 29 St Nyc| B00256|\n",
      "|7/1/2014|0:00|141 W 28 St Nyc| B00256|\n",
      "+--------+----+---------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|   Column|Nbr of Null|\n",
      "+---------+-----------+\n",
      "|     Date|          0|\n",
      "|     Time|          0|\n",
      "|PU_Adress|          0|\n",
      "|  Base_No|          0|\n",
      "+---------+-----------+\n",
      "\n",
      "Initial number of rows: 256519\n",
      "************\n",
      "\n",
      "4: Dial7_B00887.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('State', 'string'), ('PuFrom', 'string'), ('Address', 'string'), ('Street', 'string')]\n",
      "+----------+-----+--------------------+---------+-------+--------------------+\n",
      "|      Date| Time|               State|   PuFrom|Address|              Street|\n",
      "+----------+-----+--------------------+---------+-------+--------------------+\n",
      "|2014.07.06|14:30|NY               ...|MANHATTAN|     50|MURRAY ST           |\n",
      "|2014.07.04| 7:15|NY               ...|MANHATTAN|    143|AVENUE B            |\n",
      "|2014.07.05| 5:45|NY               ...|MANHATTAN|    125|CHRISTOPHER ST      |\n",
      "+----------+-----+--------------------+---------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-----------+\n",
      "| Column|Nbr of Null|\n",
      "+-------+-----------+\n",
      "|   Date|          0|\n",
      "|   Time|          0|\n",
      "|  State|          0|\n",
      "| PuFrom|          0|\n",
      "|Address|          0|\n",
      "| Street|          0|\n",
      "+-------+-----------+\n",
      "\n",
      "Initial number of rows: 194992\n",
      "************\n",
      "\n",
      "5: Diplo_B01196.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('PU_Address', 'string')]\n",
      "+--------+-----------+--------------------+\n",
      "|    Date|       Time|          PU_Address|\n",
      "+--------+-----------+--------------------+\n",
      "|7/1/2014|12:00:00 AM| 2396 Valentine A...|\n",
      "|7/1/2014|12:01:00 AM| 1859 Walton Ave ...|\n",
      "|7/1/2014|12:02:00 AM| 2431 Jerome Ave ...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----------+\n",
      "|    Column|Nbr of Null|\n",
      "+----------+-----------+\n",
      "|      Date|          0|\n",
      "|      Time|          0|\n",
      "|PU_Address|          0|\n",
      "+----------+-----------+\n",
      "\n",
      "Initial number of rows: 98550\n",
      "************\n",
      "\n",
      "6: Federal_02216.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('PU_Address2', 'string'), ('DO_Address', 'string'), ('Routing_Details', 'string'), ('PU_Address5', 'string'), ('Status', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Time, PU_Address, DO_Address, Routing Details, PU_Address, Status\n",
      " Schema: Date, Time, PU_Address2, DO_Address, Routing Details, PU_Address5, Status\n",
      "Expected: PU_Address2 but found: PU_Address\n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Federal_02216.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|      Date|    Time|         PU_Address2|          DO_Address|     Routing_Details|         PU_Address5|   Status|\n",
      "+----------+--------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|07/01/2014|07:15 AM|Brooklyn Museum, ...|1 Brookdale Plaza...|PU: Brooklyn Muse...|Brooklyn Museum, ...|Cancelled|\n",
      "|07/01/2014|07:30 AM|33 Robert Dr., Sh...|John F Kennedy In...|PU: 33 Robert Dr....|33 Robert Dr., Sh...|  Arrived|\n",
      "|07/01/2014|08:00 AM|60 Glenmore Ave.,...|2171 Nostrand Ave...|PU: 60 Glenmore A...|60 Glenmore Ave.,...| Assigned|\n",
      "+----------+--------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           Date|          0|\n",
      "|           Time|          0|\n",
      "|    PU_Address2|          0|\n",
      "|     DO_Address|          0|\n",
      "|Routing_Details|          0|\n",
      "|    PU_Address5|          0|\n",
      "|         Status|          0|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 276\n",
      "************\n",
      "\n",
      "7: Firstclass_B01536.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PICK_UP_ADDRESS', 'string')]\n",
      "+--------+-----------+--------------------+\n",
      "|    DATE|       TIME|     PICK_UP_ADDRESS|\n",
      "+--------+-----------+--------------------+\n",
      "|7/1/2014|12:02:00 AM| 5360 Broadway Ki...|\n",
      "|7/1/2014|12:02:00 AM|    546 Isham St NYC|\n",
      "|7/1/2014|12:03:00 AM| 234 Bradhurst Av...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           DATE|          0|\n",
      "|           TIME|          0|\n",
      "|PICK_UP_ADDRESS|          0|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 166769\n",
      "************\n",
      "\n",
      "8: Highclass_B01717.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PU_Address', 'string')]\n",
      "+--------+-----------+--------------------+\n",
      "|    DATE|       TIME|          PU_Address|\n",
      "+--------+-----------+--------------------+\n",
      "|7/1/2014|12:00:00 AM| 2976 Marion Ave ...|\n",
      "|7/1/2014|12:01:00 AM| 780 Grand Concou...|\n",
      "|7/1/2014|12:01:00 AM| 105 Elliot Pl Hi...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|    Column|Nbr of Null|\n",
      "+----------+-----------+\n",
      "|      DATE|          0|\n",
      "|      TIME|          0|\n",
      "|PU_Address|          0|\n",
      "+----------+-----------+\n",
      "\n",
      "Initial number of rows: 151925\n",
      "************\n",
      "\n",
      "9: Lyft_B02510.csv\n",
      "[('time_of_trip', 'string'), ('start_lat', 'string'), ('start_lng', 'string')]\n",
      "+---------------+---------+---------+\n",
      "|   time_of_trip|start_lat|start_lng|\n",
      "+---------------+---------+---------+\n",
      "|  9/4/2014 9:51| 40.64705|-73.77988|\n",
      "|8/27/2014 21:13| 40.74916|-73.98373|\n",
      "| 9/4/2014 14:16| 40.64065|-73.97594|\n",
      "+---------------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|      Column|Nbr of Null|\n",
      "+------------+-----------+\n",
      "|time_of_trip|          0|\n",
      "|   start_lat|          0|\n",
      "|   start_lng|          0|\n",
      "+------------+-----------+\n",
      "\n",
      "Initial number of rows: 267700\n",
      "************\n",
      "\n",
      "10: Prestige_B01338.csv\n",
      "[('DATE', 'string'), ('TIME', 'string'), ('PICK_UP_ADDRESS', 'string')]\n",
      "+--------+-----------+--------------------+\n",
      "|    DATE|       TIME|     PICK_UP_ADDRESS|\n",
      "+--------+-----------+--------------------+\n",
      "|7/1/2014|12:00:00 AM| 2557 Marion Ave ...|\n",
      "|7/1/2014|12:00:00 AM| 45 E Mosholu Pkw...|\n",
      "|7/1/2014|12:00:00 AM| 458 E 143rd St M...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|         Column|Nbr of Null|\n",
      "+---------------+-----------+\n",
      "|           DATE|          0|\n",
      "|           TIME|          0|\n",
      "|PICK_UP_ADDRESS|          0|\n",
      "+---------------+-----------+\n",
      "\n",
      "Initial number of rows: 320641\n",
      "************\n",
      "\n",
      "11: Skyline_B00111.csv\n",
      "[('Date', 'string'), ('Time', 'string'), ('Street_Address', 'string'), ('City_State', 'string')]\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "|    Date|          Time|      Street_Address|          City_State|\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "|7/1/2014|    20:27     |    622 THIRD AV ...|     M           ...|\n",
      "|7/1/2014|    21:04     |     E 77TH ST   ...|     M           ...|\n",
      "|7/1/2014|    22:20     |    67 WEST PALIS...|    PALISADES PAR...|\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 414:>                                                        (0 + 3) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|        Column|Nbr of Null|\n",
      "+--------------+-----------+\n",
      "|          Date|          0|\n",
      "|          Time|          0|\n",
      "|Street_Address|          0|\n",
      "|    City_State|          0|\n",
      "+--------------+-----------+\n",
      "\n",
      "Initial number of rows: 127696\n",
      "************\n",
      "\n",
      "12: other-FHV-data-jan-aug-2015.csv\n",
      "[('Base_Number', 'string'), ('Base_Name', 'string'), ('Pick_Up_Date', 'string'), ('Number_of_Trips', 'string'), ('Number_of_Vehicles', 'string')]\n",
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "|Base_Number|           Base_Name|Pick_Up_Date|Number_of_Trips|Number_of_Vehicles|\n",
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "|     B00013|   LOVE CORP CAR INC|  01/01/2015|            26 |               17 |\n",
      "|     B00014| NY ONE CORP CAR INC|  01/01/2015|            45 |               24 |\n",
      "|     B00029|COMMUNITY CAR SVC...|  01/01/2015|           731 |               36 |\n",
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|            Column|Nbr of Null|\n",
      "+------------------+-----------+\n",
      "|       Base_Number|          0|\n",
      "|         Base_Name|          0|\n",
      "|      Pick_Up_Date|          0|\n",
      "|   Number_of_Trips|          0|\n",
      "|Number_of_Vehicles|          0|\n",
      "+------------------+-----------+\n",
      "\n",
      "Initial number of rows: 26181\n",
      "************\n",
      "\n",
      "13: Uber-Jan-Feb-FOIL.csv\n",
      "[('dispatching_base_number', 'string'), ('date', 'string'), ('active_vehicles', 'string'), ('trips', 'string')]\n",
      "+-----------------------+--------+---------------+-----+\n",
      "|dispatching_base_number|    date|active_vehicles|trips|\n",
      "+-----------------------+--------+---------------+-----+\n",
      "|                 B02512|1/1/2015|            190| 1132|\n",
      "|                 B02765|1/1/2015|            225| 1765|\n",
      "|                 B02764|1/1/2015|           3427|29421|\n",
      "+-----------------------+--------+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-----------+\n",
      "|              Column|Nbr of Null|\n",
      "+--------------------+-----------+\n",
      "|dispatching_base_...|          0|\n",
      "|                date|          0|\n",
      "|     active_vehicles|          0|\n",
      "|               trips|          0|\n",
      "+--------------------+-----------+\n",
      "\n",
      "Initial number of rows: 354\n",
      "************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "all_columns = []\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' not in key:\n",
    "        print(f'{x}: {key}')\n",
    "        print(df_list[key].dtypes)\n",
    "        df_list[key].show(3)\n",
    "        count_null_values_in_df(df_list[key])\n",
    "        all_columns.append(df_list[key].schema.names)\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d406cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A unique list of column names:\n",
      " {'locationid', 'pick_up_address', 'trips', 'street', 'base_number', 'address', 'start_lat', 'active_vehicles', 'pick_up_date', 'street_address', 'base_name', 'routing_details', 'city_state', 'pu_adress', 'do_address', 'zone', 'state', 'pu_address2', 'time', 'date', 'time_of_trip', 'base_no', 'pufrom', 'number_of_trips', 'start_lng', 'pu_address5', 'dispatching_base_number', 'status', 'pu_address', 'borough', 'number_of_vehicles'}\n"
     ]
    }
   ],
   "source": [
    "all_columns = [val for sublist in all_columns for val in sublist]\n",
    "all_columns = [x.strip().lower() for x in all_columns]\n",
    "all_columns = set(all_columns)\n",
    "print(f\"A unique list of column names:\\n {all_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6a09f",
   "metadata": {},
   "source": [
    "### Save cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e590d519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi-zone-lookup.parquet\n",
      "American_B01362.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 452:============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carmel_B00256.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dial7_B00887.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diplo_B01196.parquet\n",
      "Federal_02216.parquet\n",
      "Firstclass_B01536.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Time, PU_Address, DO_Address, Routing Details, PU_Address, Status\n",
      " Schema: Date, Time, PU_Address2, DO_Address, Routing Details, PU_Address5, Status\n",
      "Expected: PU_Address2 but found: PU_Address\n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Federal_02216.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highclass_B01717.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 458:>                                                        (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyft_B02510.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prestige_B01338.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skyline_B00111.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 461:===================>                                     (1 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other-FHV-data-jan-aug-2015.parquet\n",
      "Uber-Jan-Feb-FOIL.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/other-FHV-data'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' not in key:\n",
    "        file = os.path.splitext(key)[0]+\".parquet\"\n",
    "        print(file)\n",
    "        df_list[key].write.mode(\"overwrite\").parquet(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe27129",
   "metadata": {},
   "source": [
    "### Preparing a unified file for FHV questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cd0eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------------+\n",
      "|DATE|TIME|PICK_UP_ADDRESS|\n",
      "+----+----+---------------+\n",
      "+----+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = df_list['American_B01362.csv'].schema.names\n",
    "mySchema = StructType([StructField(c, StringType()) for c in col_names])\n",
    "FHV_10_companies_data = SparkSession(sc).createDataFrame(data=[], schema=mySchema)\n",
    "FHV_10_companies_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d482e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91712"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.'American_B01362.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(df_list['American_B01362.csv'])\n",
    "df_list['American_B01362.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fef532e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256519"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 'Carmel_B00256.csv'\n",
    "tmp = df_list['Carmel_B00256.csv'].drop('Base_No')\n",
    "tmp = tmp.withColumnRenamed(\"PU_Adress\", \"PICK_UP_ADDRESS\")\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Carmel_B00256.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2947702c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194992"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 'Dial7_B00887.csv'\n",
    "columns_to_drop = ['State','PuFrom', 'Address']\n",
    "tmp = df_list['Dial7_B00887.csv'].drop(*columns_to_drop)\n",
    "tmp = tmp.withColumnRenamed(\"Street\", \"PICK_UP_ADDRESS\")\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Dial7_B00887.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dd1fa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98550"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 'Diplo_B01196.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(\\\n",
    "df_list['Diplo_B01196.csv'].withColumnRenamed(\"PU_Address\", \"PICK_UP_ADDRESS\"))\n",
    "df_list['Diplo_B01196.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a361799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 'Federal_02216.csv'\n",
    "columns_to_drop = ['DO_Address','Routing_Details', 'Status', 'PU_Address2']\n",
    "tmp = df_list['Federal_02216.csv'].drop(*columns_to_drop)\n",
    "tmp = tmp.withColumnRenamed(\"PU_Address5\", \"PICK_UP_ADDRESS\")\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Federal_02216.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b2dbd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166769"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.'Firstclass_B01536.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(df_list['Firstclass_B01536.csv'])\n",
    "df_list['Firstclass_B01536.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4f5a43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151925"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.'Highclass_B01717.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(\\\n",
    "df_list['Highclass_B01717.csv'].withColumnRenamed(\"PU_Address\", \"PICK_UP_ADDRESS\"))\n",
    "df_list['Highclass_B01717.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca16efde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+\n",
      "|   time_of_trip|start_lat|start_lng|\n",
      "+---------------+---------+---------+\n",
      "|  9/4/2014 9:51| 40.64705|-73.77988|\n",
      "|8/27/2014 21:13| 40.74916|-73.98373|\n",
      "| 9/4/2014 14:16| 40.64065|-73.97594|\n",
      "+---------------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.'Lyft_B02510.csv'\n",
    "df_list['Lyft_B02510.csv'].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d4c5e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127696"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.'Skyline_B00111.csv'\n",
    "tmp = df_list['Skyline_B00111.csv'].drop('City_State')\n",
    "tmp = tmp.withColumnRenamed(\"Street_Address\", \"PICK_UP_ADDRESS\")\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Skyline_B00111.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d127fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320641"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10.'Prestige_B01338.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(df_list['Prestige_B01338.csv'])\n",
    "df_list['Prestige_B01338.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb1295be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409080"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FHV_10_companies_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86d59c",
   "metadata": {},
   "source": [
    "### To read parquet file use the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17c4a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:56:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Time, PU_Address\n",
      " Schema: Date, Time, PU_Address5\n",
      "Expected: PU_Address5 but found: PU_Address\n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Federal_02216.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/other-FHV-data'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "FHV_file = path+'FHV_10_companies_data.parquet'\n",
    "FHV_10_companies_data.write.mode(\"overwrite\").parquet(FHV_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f829ce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      " |-- PICK_UP_ADDRESS: string (nullable = true)\n",
      "\n",
      "+--------+----+---------------+\n",
      "|    DATE|TIME|PICK_UP_ADDRESS|\n",
      "+--------+----+---------------+\n",
      "|7/1/2014|0:00|260 W 44 St NYC|\n",
      "|7/1/2014|0:00|125 W 29 St Nyc|\n",
      "|7/1/2014|0:00|141 W 28 St Nyc|\n",
      "+--------+----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1409080"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FHV_10_Co = spark_session.read.parquet(FHV_file)\n",
    "FHV_10_Co.printSchema()\n",
    "FHV_10_Co.show(3)\n",
    "FHV_10_Co.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25fcb190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark closed!\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "print(\"Spark closed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
